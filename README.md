# llm-vectorized-malicious-prompt-detector
Detects many-shot jailbreaking in LLMs using vector embeddings. It compares incoming prompts against a database of known vulnerabilities. If similarity exceeds a set threshold, the prompt is blocked, enhancing LLM security.

# Acknowledgements
The base of this repo was developed from [abhinav-upadhyay's](https://github.com/abhinav-upadhyay) h[hatgpt_plugins base project](ttps://github.com/abhinav-upadhyay/chatgpt_plugins). 
