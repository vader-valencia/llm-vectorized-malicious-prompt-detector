# llm-vectorized-malicious-prompt-detector
Detects many-shot jailbreaking in LLMs using vector embeddings. It compares incoming prompts against a database of known vulnerabilities. If similarity exceeds a set threshold, the prompt is blocked, enhancing LLM security.
